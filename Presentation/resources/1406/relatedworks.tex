In this section, we briefly discuss existing models of fixation prediction and salient object segmentation. We also discuss the relationship of salient object to generic object segmentation such as CPMC \cite{carreira2010constrained, li2010object}.  Finally, we review relevant research pertaining to dataset bias.

\subsection{Fixation prediction}
The problem of fixation based bottom-up saliency is first introduced to computer vision community by \cite{itti1998model}.  The goal of this type of models is to compute a ``saliency map'' that simulates the eye movement behaviors of human.  Patch-based \cite{itti1998model, bruce2005saliency, harel2006graph, hou2008dynamic, zhang2008sun} or pixel-based \cite{hou2012image, garcia2012relationship} features are often used in these models, followed by a local or global interaction step that re-weight or re-normalize features saliency values.

To quantitatively evaluate the performance of different fixation algorithms, ROC Area Under the Curve (AUC) is often used to compare a saliency map against human eye fixations. One of the first systematic datasets in fixation prediction was introduced in \cite{bruce2005saliency}.  In this paper, Bruce \emph{et al.} recorded eye fixation data from $21$ subjects on $120$ natural images.  In a more recent paper \cite{judd2009learning}, Judd \emph{et al.} introduced a much larger dataset with $1003$ images and $15$ subjects.

Due to the nature of eye tracking experiments, the error of recorded fixation locations can go up to $1^\circ$, or over $30$ pixels in a typical setting.  Therefore, there is no need to generate a pixel-accurate saliency map to match human data.  In fact, as pointed out in \cite{hou2012image}, blurring a saliency map can often increase its AUC score.


\subsection{Salient object segmentation}
It is not an easy task to directly use the blurry saliency map from a fixation prediction algorithm.  As an alternative, Liu \emph{et al.} \cite{liu2007learning} proposed the MSRA-5000 dataset with bounding boxes on the salient objects. Following the idea of ``object-based'' saliency, Achanta \emph{et al.} \cite{achanta2009frequency} further labeled $1000$ images from MSRA-5000 with pixel-accurate object silhouette masks.  Their paper showed that existing fixation algorithms perform poorly if benchmarked F-measures of PR curve.  Inspired by this new dataset, a line of papers has proposed \cite{cheng2011global, perazzi2012saliency, margolin2013makes} to tackle this new challenge of predicting full-resolution masks of salient objects.  An overview of the characteristics and performances of salient object algorithms can be found in a recent review \cite{borji2012salient} by Borji \emph{et al.}

Despite the deep connections between the problems of fixation prediction and object segmentation, there is a discomforting isolation between major computational models of the two types.  Salient object segmentation algorithms have developed a set of techniques that have little overlapping with fixation prediction models.  This is mainly due to a series of differences in the ground-truth and evaluation procedures.  A typical fixation ground-truth contains several fixation dots, while a salient object ground-truth usually have one or several positive regions composed of thousands of pixels.  Having different priors of sparsity significantly limited the model of one type to have good performance on tasks of the other type.

\subsection{Objectness, object proposal, and foreground segments}\label{sec:reviewObjectness}
In the field of object recognition, researchers are interested in finding objects independent of their classes~\cite{BingObj2014}.  Alexe \emph{et al.}~\cite{alexe2012measuring} used a combination of low/mid level image cues to measure the ``objectness'' of a bounding box.  Other models, such as CPMC \cite{carreira2010constrained,li2010object} and Object Proposal \cite{endres2010category}, generate segmentations of candidate objects without relying on category specific information.  The obtained ``foreground segments'', or ``object proposals'' are then ranked or scored to give a rough estimate of the objects in the scene.

The role of a scoring/ranking function in the aforementioned literature shares a lot of similarities with the notion of saliency.  In fact, \cite{alexe2012measuring} used saliency maps as a main feature for predicting objectness.  In Sec.~\ref{sec:model}, we propose a model based on the foreground segmentations generated by CPMC.  One fundamental difference between these methods to visual saliency is that an object detector is often exhaustive -- it looks for \emph{all} objects in the image irrespective of their saliency value.  In comparison, a salient object detector aims at enumerating a subset of objects that exceed certain saliency threshold.  As we will discuss in Sec.~\ref{sec:model}, an object model, such as CPMC offers a ranking of its candidate foreground proposals.  However, the top ranked (e.g. first 200) segments do not always correspond to salient objects or their parts.


\subsection{Datasets and dataset bias}\label{sec:reviewBias}
Recently, researchers started to quantitatively analyze dataset bias and their detrimental effect in benchmarking.  Dataset bias arises from the selection of images \cite{torralba2011unbiased}, as well as the annotation process \cite{tatler2005visual}.  In the field of visual saliency analysis, the most significant bias is \emph{center bias}.  It refers to the tendency that subjects look more often at the center of the screen \cite{tatler2005visual}.  This phenomenon might be partly due to experimental constraints that a subject's head being on a chin-rest during the fixation experiment, and partly due to the photographer's preference to align objects at the center of the photos.

Center bias has been shown to have a significant influence on benchmark scores \cite{judd2009learning, zhang2008sun}. Fixation models either use it explicitly \cite{judd2009learning}; or implicitly by padding the borders of a saliency map \cite{harel2006graph, hou2008dynamic, zhang2008sun}.  To make a fair evaluation of the algorithm's true prediction power, Tatler \cite{tatler2005visual} proposed a shuffled-AUC (s-AUC) score to normalize the effect of center-bias.  In s-AUC, positive samples are taken from the fixations of the test image, whereas the negative samples are from all fixations across all other images.














